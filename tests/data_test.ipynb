{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": "# Sociopathit Data Utilities - Comprehensive Test Suite\n\nThis notebook demonstrates and tests all data utility modules in the sociopathit package.\n\n## ðŸ“š Table of Contents\n\nClick to jump to any section:\n\n1. **[Setup & Initialization](#setup)** - Environment configuration\n2. **[File Discovery](#discovery)** - Finding and scanning data files\n3. **[Data Loading](#loading)** - Loading surveys and Stata files\n4. **[Metadata Utilities](#metadata)** - Survey summaries and structure\n5. **[Longitudinal Data](#longitudinal)** - Panel data management\n6. **[Harmonization](#preparation)** - Data preparation and cleaning\n\n---\n\n## ðŸš€ Quick Start\n\n```python\n# Example: Load and harmonize multiple survey waves\nfrom sociopathit.data.loading import load_all_surveys\nfrom sociopathit.data.preparation import build_harmonized_dataset\n\n# Load all surveys\nsurveys = load_all_surveys('data/', target_vars=['age', 'income', 'satisfaction'])\n\n# Build harmonized long-form dataset\ndf_long = build_harmonized_dataset(surveys, target_vars=['age', 'income', 'satisfaction'])\n```\n\n---"
  },
  {
   "cell_type": "code",
   "id": "setup",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T22:49:32.737039Z",
     "start_time": "2025-10-16T22:49:32.315465Z"
    }
   },
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "# Locate and add the package root\n",
    "cwd = Path.cwd().resolve()\n",
    "for parent in [cwd, *cwd.parents]:\n",
    "    if (parent / \"sociopathit\").exists():\n",
    "        ROOT = parent\n",
    "        break\n",
    "else:\n",
    "    raise FileNotFoundError(\"Could not locate the sociopathit package root.\")\n",
    "\n",
    "# Add to sys.path for imports\n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(ROOT))\n",
    "\n",
    "print(f\"Added to sys.path: {ROOT}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added to sys.path: C:\\Users\\alecw\\OneDrive - University of Toronto\\Directives\\GITTYSBURG\\sociopathit\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "section_discovery",
   "metadata": {},
   "source": "<a id='discovery'></a>\n# 2. File Discovery & Path Handling\n\nTest functions for discovering and scanning data files in directory structures."
  },
  {
   "cell_type": "code",
   "id": "test_discovery_imports",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T22:49:45.234035Z",
     "start_time": "2025-10-16T22:49:32.757276Z"
    }
   },
   "source": [
    "import importlib\n",
    "from sociopathit.data import discovery as discovery_module\n",
    "importlib.reload(discovery_module)\n",
    "from sociopathit.data.discovery import resolve_qwels_root, discover_data, find_latest_wave\n",
    "\n",
    "print(\"Discovery module loaded successfully\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discovery module loaded successfully\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "test_resolve_root",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T22:49:45.548614Z",
     "start_time": "2025-10-16T22:49:45.542282Z"
    }
   },
   "source": [
    "# Test 1: resolve_qwels_root\n",
    "print(\"=\" * 60)\n",
    "print(\"TEST 1: Resolve QWELS Root Directory\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    root = resolve_qwels_root()\n",
    "    print(f\"Found root directory: {root}\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Could not find root (expected if not in QWELS structure): {e}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TEST 1: Resolve QWELS Root Directory\n",
      "============================================================\n",
      "Found root directory: C:\\Users\\alecw\\OneDrive - University of Toronto\\Directives\\QWELS\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "test_discover_data",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T22:49:45.654631Z",
     "start_time": "2025-10-16T22:49:45.582553Z"
    }
   },
   "source": "# Test 2: discover_data with simulated data\nprint(\"=\" * 60)\nprint(\"TEST 2: Discover Data Files\")\nprint(\"=\" * 60)\n\n# Create temporary directory with test data\nwith TemporaryDirectory() as tmpdir:\n    tmpdir = Path(tmpdir)\n    \n    # Create some test CSV files\n    for year in [2020, 2021, 2022]:\n        df_test = pd.DataFrame({\n            'pid': range(1, 101),\n            'age': np.random.randint(18, 65, 100),\n            'income': np.random.randint(30000, 100000, 100),\n            'satisfaction': np.random.choice(['Low', 'Medium', 'High'], 100)\n        })\n        df_test.to_csv(tmpdir / f\"survey_{year}.csv\", index=False)\n    \n    # Create a test file (should be excluded)\n    df_test.to_csv(tmpdir / \"survey_2023_test.csv\", index=False)\n    \n    # Discover data\n    data = discover_data(\n        tmpdir,\n        file_types=['csv'],\n        exclude_patterns=['*_test*'],\n        recursive=False\n    )\n    \n    print(f\"\\nDiscovered {len(data)} datasets:\")\n    for survey_id, df in data.items():\n        print(f\"  - {survey_id}: {len(df)} rows, {len(df.columns)} columns\")\n        print(f\"    Columns: {list(df.columns)}\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TEST 2: Discover Data Files\n",
      "============================================================\n",
      "âœ“ Loaded 3 datasets\n",
      "  File types: csv=3\n",
      "âš  Skipped 1 files\n",
      "  - survey_2023_test.csv: matched exclusion pattern\n",
      "\n",
      "Discovered 3 datasets:\n",
      "  - survey_2020: 100 rows, 4 columns\n",
      "    Columns: ['pid', 'age', 'income', 'satisfaction']\n",
      "  - survey_2021: 100 rows, 4 columns\n",
      "    Columns: ['pid', 'age', 'income', 'satisfaction']\n",
      "  - survey_2022: 100 rows, 4 columns\n",
      "    Columns: ['pid', 'age', 'income', 'satisfaction']\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "test_find_latest_wave",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T22:49:45.672379Z",
     "start_time": "2025-10-16T22:49:45.667744Z"
    }
   },
   "source": [
    "# Test 3: find_latest_wave\n",
    "print(\"=\" * 60)\n",
    "print(\"TEST 3: Find Latest Wave\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create test data with different waves\n",
    "test_data = {\n",
    "    'survey_2020': pd.DataFrame({'pid': range(1, 51), 'value': np.random.random(50)}),\n",
    "    'survey_2021': pd.DataFrame({'pid': range(1, 51), 'value': np.random.random(50)}),\n",
    "    'survey_2022': pd.DataFrame({'pid': range(1, 51), 'value': np.random.random(50)}),\n",
    "}\n",
    "\n",
    "wave_info = find_latest_wave(test_data, name_pattern=r'(\\d{4})')\n",
    "\n",
    "print(f\"\\nLatest wave: {wave_info['latest'][0]}\")\n",
    "print(f\"Earliest wave: {wave_info['earliest'][0]}\")\n",
    "print(f\"\\nSorted waves:\")\n",
    "for survey_id, year in wave_info['sorted_waves']:\n",
    "    print(f\"  {survey_id} ({year}) -> {wave_info['wave_labels'][survey_id]}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TEST 3: Find Latest Wave\n",
      "============================================================\n",
      "\n",
      "Latest wave: survey_2022\n",
      "Earliest wave: survey_2020\n",
      "\n",
      "Sorted waves:\n",
      "  survey_2020 (2020) -> wave_1\n",
      "  survey_2021 (2021) -> wave_2\n",
      "  survey_2022 (2022) -> wave_3\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "section_loading",
   "metadata": {},
   "source": "<a id='loading'></a>\n# 3. Data Loading & Preparation\n\nTest functions for loading survey data and Stata (.dta) files with automatic preprocessing."
  },
  {
   "cell_type": "code",
   "id": "test_loading_imports",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T22:49:45.704523Z",
     "start_time": "2025-10-16T22:49:45.700212Z"
    }
   },
   "source": [
    "import importlib\n",
    "from sociopathit.data import loading as loading_module\n",
    "importlib.reload(loading_module)\n",
    "from sociopathit.data.loading import load_stata, load_all_surveys\n",
    "\n",
    "print(\"Loading module loaded successfully\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading module loaded successfully\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "test_load_all_surveys",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T22:49:45.798257Z",
     "start_time": "2025-10-16T22:49:45.733483Z"
    }
   },
   "source": [
    "# Test 4: load_all_surveys\n",
    "print(\"=\" * 60)\n",
    "print(\"TEST 4: Load All Surveys\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "with TemporaryDirectory() as tmpdir:\n",
    "    tmpdir = Path(tmpdir)\n",
    "    \n",
    "    # Create test data files\n",
    "    for year in [2020, 2021, 2022]:\n",
    "        df_test = pd.DataFrame({\n",
    "            'PID': range(1, 101),\n",
    "            'Age': np.random.randint(18, 65, 100),\n",
    "            'Income': np.random.randint(30000, 100000, 100),\n",
    "            'Job Satisfaction': np.random.choice(['Low', 'Medium', 'High'], 100),\n",
    "            'Extra_Col': np.random.random(100)\n",
    "        })\n",
    "        df_test.to_csv(tmpdir / f\"survey_{year}.csv\", index=False)\n",
    "    \n",
    "    # Load all surveys\n",
    "    surveys = load_all_surveys(\n",
    "        tmpdir,\n",
    "        file_extensions=['.csv'],\n",
    "        target_vars=['pid', 'age', 'income', 'job_satisfaction']\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nLoaded {len(surveys)} surveys:\")\n",
    "    for survey_id, df in surveys.items():\n",
    "        print(f\"\\n{survey_id}:\")\n",
    "        print(f\"  Shape: {df.shape}\")\n",
    "        print(f\"  Columns: {list(df.columns)}\")\n",
    "        print(f\"  Source: {df.attrs.get('source_file', 'N/A')}\")\n",
    "        print(f\"  Sample data:\\n{df.head(3)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TEST 4: Load All Surveys\n",
      "============================================================\n",
      "âœ“ Successfully loaded 3 surveys\n",
      "\n",
      "Loaded 3 surveys:\n",
      "\n",
      "survey_2020:\n",
      "  Shape: (100, 4)\n",
      "  Columns: ['pid', 'age', 'income', 'job_satisfaction']\n",
      "  Source: C:\\Users\\alecw\\AppData\\Local\\Temp\\tmp5hfc_w_1\\survey_2020.csv\n",
      "  Sample data:\n",
      "   pid  age  income job_satisfaction\n",
      "0    1   34   49537             High\n",
      "1    2   25   40385           Medium\n",
      "2    3   44   74355           Medium\n",
      "\n",
      "survey_2021:\n",
      "  Shape: (100, 4)\n",
      "  Columns: ['pid', 'age', 'income', 'job_satisfaction']\n",
      "  Source: C:\\Users\\alecw\\AppData\\Local\\Temp\\tmp5hfc_w_1\\survey_2021.csv\n",
      "  Sample data:\n",
      "   pid  age  income job_satisfaction\n",
      "0    1   41   90622           Medium\n",
      "1    2   31   98930              Low\n",
      "2    3   20   83911              Low\n",
      "\n",
      "survey_2022:\n",
      "  Shape: (100, 4)\n",
      "  Columns: ['pid', 'age', 'income', 'job_satisfaction']\n",
      "  Source: C:\\Users\\alecw\\AppData\\Local\\Temp\\tmp5hfc_w_1\\survey_2022.csv\n",
      "  Sample data:\n",
      "   pid  age  income job_satisfaction\n",
      "0    1   18   84197           Medium\n",
      "1    2   41   86308             High\n",
      "2    3   46   66894           Medium\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "section_metadata",
   "metadata": {},
   "source": "<a id='metadata'></a>\n# 4. Metadata & Structure Utilities\n\nTest functions for extracting metadata, wave information, and survey structure analysis."
  },
  {
   "cell_type": "code",
   "id": "test_metadata_imports",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T22:49:45.815461Z",
     "start_time": "2025-10-16T22:49:45.812055Z"
    }
   },
   "source": [
    "import importlib\n",
    "from sociopathit.data import metadata as metadata_module\n",
    "importlib.reload(metadata_module)\n",
    "from sociopathit.data.metadata import summarize_surveys, extract_wave_info, get_id_vars\n",
    "\n",
    "print(\"Metadata module loaded successfully\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata module loaded successfully\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "test_summarize_surveys",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T22:49:45.854413Z",
     "start_time": "2025-10-16T22:49:45.844452Z"
    }
   },
   "source": [
    "# Test 5: summarize_surveys\n",
    "print(\"=\" * 60)\n",
    "print(\"TEST 5: Summarize Surveys\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create test data\n",
    "test_surveys = {\n",
    "    'survey_2020': pd.DataFrame({\n",
    "        'pid': range(1, 1001),\n",
    "        'age': np.random.randint(18, 65, 1000),\n",
    "        'income': np.random.randint(30000, 100000, 1000),\n",
    "        'jobsat': np.random.randint(1, 8, 1000),\n",
    "        'satguess': np.random.randint(1, 8, 1000)\n",
    "    }),\n",
    "    'survey_2021': pd.DataFrame({\n",
    "        'pid': range(1, 1201),\n",
    "        'age': np.random.randint(18, 65, 1200),\n",
    "        'income': np.random.randint(30000, 100000, 1200),\n",
    "        'jobsat': np.random.randint(1, 8, 1200),\n",
    "        'perceivedinequality': np.random.randint(1, 6, 1200)\n",
    "    }),\n",
    "}\n",
    "\n",
    "# Add ID column to attrs\n",
    "test_surveys['survey_2020'].attrs['id_column'] = 'pid'\n",
    "test_surveys['survey_2021'].attrs['id_column'] = 'pid'\n",
    "\n",
    "summary = summarize_surveys(\n",
    "    test_surveys,\n",
    "    key_constructs=['age', 'income', 'jobsat', 'satguess', 'perceivedinequality']\n",
    ")\n",
    "\n",
    "print(\"\\nSurvey Summary:\")\n",
    "print(summary.to_string(index=False))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TEST 5: Summarize Surveys\n",
      "============================================================\n",
      "\n",
      "Survey Summary:\n",
      "  survey_id    N  n_vars id_column  has_age  has_income  has_jobsat  has_satguess  has_perceivedinequality\n",
      "survey_2020 1000       5       pid     True        True        True          True                    False\n",
      "survey_2021 1200       5       pid     True        True        True         False                     True\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "test_extract_wave_info",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T22:49:45.893554Z",
     "start_time": "2025-10-16T22:49:45.887948Z"
    }
   },
   "source": [
    "# Test 6: extract_wave_info\n",
    "print(\"=\" * 60)\n",
    "print(\"TEST 6: Extract Wave Information\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "survey_ids = [\n",
    "    'qwels_jan_2020',\n",
    "    'qwels_jun_2020', \n",
    "    'qwels_jan_2021',\n",
    "    'qwels_dec_2021',\n",
    "    'qwels_mar_2022'\n",
    "]\n",
    "\n",
    "wave_info = extract_wave_info(survey_ids)\n",
    "\n",
    "print(\"\\nExtracted Wave Information:\")\n",
    "print(wave_info.to_string(index=False))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TEST 6: Extract Wave Information\n",
      "============================================================\n",
      "\n",
      "Extracted Wave Information:\n",
      "     survey_id  year month date wave_number wave_id\n",
      "qwels_jan_2020  2020   jan None        None  wave_1\n",
      "qwels_jun_2020  2020   jun None        None  wave_2\n",
      "qwels_jan_2021  2021   jan None        None  wave_3\n",
      "qwels_dec_2021  2021   dec None        None  wave_4\n",
      "qwels_mar_2022  2022   mar None        None  wave_5\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "test_get_id_vars",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T22:49:45.928444Z",
     "start_time": "2025-10-16T22:49:45.922235Z"
    }
   },
   "source": [
    "# Test 7: get_id_vars\n",
    "print(\"=\" * 60)\n",
    "print(\"TEST 7: Identify ID Variables\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test with explicit ID column\n",
    "df_test1 = pd.DataFrame({\n",
    "    'pid': range(1, 101),\n",
    "    'age': np.random.randint(18, 65, 100),\n",
    "    'income': np.random.randint(30000, 100000, 100)\n",
    "})\n",
    "\n",
    "id_col = get_id_vars(df_test1)\n",
    "print(f\"\\nTest 1 - Explicit ID column:\")\n",
    "print(f\"  Identified ID: {id_col}\")\n",
    "\n",
    "# Test with first column as ID\n",
    "df_test2 = pd.DataFrame({\n",
    "    'respondent_number': range(1, 101),\n",
    "    'age': np.random.randint(18, 65, 100),\n",
    "    'income': np.random.randint(30000, 100000, 100)\n",
    "})\n",
    "\n",
    "id_col = get_id_vars(df_test2)\n",
    "print(f\"\\nTest 2 - First column as ID:\")\n",
    "print(f\"  Identified ID: {id_col}\")\n",
    "\n",
    "# Test return_all\n",
    "all_ids = get_id_vars(df_test1, return_all=True)\n",
    "print(f\"\\nTest 3 - All potential IDs:\")\n",
    "print(f\"  All IDs: {all_ids}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TEST 7: Identify ID Variables\n",
      "============================================================\n",
      "\n",
      "Test 1 - Explicit ID column:\n",
      "  Identified ID: pid\n",
      "\n",
      "Test 2 - First column as ID:\n",
      "  Identified ID: respondent_number\n",
      "\n",
      "Test 3 - All potential IDs:\n",
      "  All IDs: ['pid']\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "id": "section_longitudinal",
   "metadata": {},
   "source": "<a id='longitudinal'></a>\n# 5. Longitudinal Data Management\n\nTest functions for detecting panel structure, aligning waves, and managing longitudinal datasets."
  },
  {
   "cell_type": "code",
   "id": "test_longitudinal_imports",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T22:49:45.965255Z",
     "start_time": "2025-10-16T22:49:45.961294Z"
    }
   },
   "source": [
    "import importlib\n",
    "from sociopathit.data import longitudinal as longitudinal_module\n",
    "importlib.reload(longitudinal_module)\n",
    "from sociopathit.data.longitudinal import detect_longitudinal, align_longitudinal_data, sort_by_wave\n",
    "\n",
    "print(\"Longitudinal module loaded successfully\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longitudinal module loaded successfully\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "test_detect_longitudinal",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T22:49:46.001865Z",
     "start_time": "2025-10-16T22:49:45.992244Z"
    }
   },
   "source": [
    "# Test 8: detect_longitudinal\n",
    "print(\"=\" * 60)\n",
    "print(\"TEST 8: Detect Longitudinal Structure\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create panel data (same individuals across waves)\n",
    "np.random.seed(42)\n",
    "shared_pids = list(range(1, 51))  # 50 individuals\n",
    "new_pids_wave2 = list(range(51, 61))  # 10 new in wave 2\n",
    "new_pids_wave3 = list(range(61, 71))  # 10 new in wave 3\n",
    "\n",
    "panel_data = {\n",
    "    'wave1': pd.DataFrame({\n",
    "        'pid': shared_pids,\n",
    "        'age': np.random.randint(18, 65, len(shared_pids)),\n",
    "        'income': np.random.randint(30000, 100000, len(shared_pids))\n",
    "    }),\n",
    "    'wave2': pd.DataFrame({\n",
    "        'pid': shared_pids + new_pids_wave2,\n",
    "        'age': np.random.randint(18, 65, len(shared_pids + new_pids_wave2)),\n",
    "        'income': np.random.randint(30000, 100000, len(shared_pids + new_pids_wave2))\n",
    "    }),\n",
    "    'wave3': pd.DataFrame({\n",
    "        'pid': shared_pids + new_pids_wave3,\n",
    "        'age': np.random.randint(18, 65, len(shared_pids + new_pids_wave3)),\n",
    "        'income': np.random.randint(30000, 100000, len(shared_pids + new_pids_wave3))\n",
    "    })\n",
    "}\n",
    "\n",
    "result = detect_longitudinal(panel_data, id_col='pid')\n",
    "\n",
    "print(f\"\\nIs longitudinal: {result['is_longitudinal']}\")\n",
    "print(f\"ID column: {result['id_column']}\")\n",
    "print(f\"Number of shared IDs: {len(result['shared_ids'])}\")\n",
    "print(f\"\\nOverlap Matrix:\")\n",
    "print(result['overlap_matrix'])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TEST 8: Detect Longitudinal Structure\n",
      "============================================================\n",
      "\n",
      "Is longitudinal: True\n",
      "ID column: pid\n",
      "Number of shared IDs: 50\n",
      "\n",
      "Overlap Matrix:\n",
      "       wave1  wave2  wave3\n",
      "wave1   50.0   50.0   50.0\n",
      "wave2   50.0   60.0   50.0\n",
      "wave3   50.0   50.0   60.0\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "id": "test_align_longitudinal",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T22:49:46.042759Z",
     "start_time": "2025-10-16T22:49:46.034091Z"
    }
   },
   "source": [
    "# Test 9: align_longitudinal_data\n",
    "print(\"=\" * 60)\n",
    "print(\"TEST 9: Align Longitudinal Data\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Use panel data from previous test\n",
    "wave_labels = {\n",
    "    'wave1': 'wave_1',\n",
    "    'wave2': 'wave_2',\n",
    "    'wave3': 'wave_3'\n",
    "}\n",
    "\n",
    "long_df = align_longitudinal_data(\n",
    "    panel_data,\n",
    "    id_col='pid',\n",
    "    wave_labels=wave_labels,\n",
    "    align_vars=['age', 'income']\n",
    ")\n",
    "\n",
    "print(f\"\\nLong-form dataset shape: {long_df.shape}\")\n",
    "print(f\"Columns: {list(long_df.columns)}\")\n",
    "print(f\"\\nSample data (first individual across waves):\")\n",
    "print(long_df[long_df['pid'] == 1])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TEST 9: Align Longitudinal Data\n",
      "============================================================\n",
      "\n",
      "Long-form dataset shape: (170, 5)\n",
      "Columns: ['pid', 'age', 'income', 'wave', 'source']\n",
      "\n",
      "Sample data (first individual across waves):\n",
      "   pid  age  income    wave source\n",
      "0    1   56   53483  wave_1  wave1\n",
      "1    1   29   51976  wave_2  wave2\n",
      "2    1   45   77333  wave_3  wave3\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "id": "test_sort_by_wave",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T22:49:47.101024Z",
     "start_time": "2025-10-16T22:49:47.091725Z"
    }
   },
   "source": [
    "# Test 10: sort_by_wave\n",
    "print(\"=\" * 60)\n",
    "print(\"TEST 10: Sort by Wave\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create unsorted long data\n",
    "unsorted_df = long_df.sample(frac=1, random_state=42)  # Shuffle\n",
    "\n",
    "print(\"Before sorting:\")\n",
    "print(unsorted_df.head(10))\n",
    "\n",
    "sorted_df = sort_by_wave(unsorted_df, id_col='pid', wave_col='wave')\n",
    "\n",
    "print(\"\\nAfter sorting:\")\n",
    "print(sorted_df.head(10))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TEST 10: Sort by Wave\n",
      "============================================================\n",
      "Before sorting:\n",
      "     pid  age  income    wave source\n",
      "139   47   39   72107  wave_2  wave2\n",
      "30    11   28   39692  wave_1  wave1\n",
      "119   40   35   34014  wave_3  wave3\n",
      "29    10   43   35600  wave_3  wave3\n",
      "144   49   19   38392  wave_1  wave1\n",
      "163   64   20   59592  wave_3  wave3\n",
      "166   67   29   32368  wave_3  wave3\n",
      "51    18   19   31016  wave_1  wave1\n",
      "105   36   61   78190  wave_1  wave1\n",
      "60    21   47   54300  wave_1  wave1\n",
      "\n",
      "After sorting:\n",
      "   pid  age  income    wave source\n",
      "0    1   56   53483  wave_1  wave1\n",
      "1    1   29   51976  wave_2  wave2\n",
      "2    1   45   77333  wave_3  wave3\n",
      "3    2   46   78555  wave_1  wave1\n",
      "4    2   51   74262  wave_2  wave2\n",
      "5    2   42   33436  wave_3  wave3\n",
      "6    3   32   47159  wave_1  wave1\n",
      "7    3   50   53776  wave_2  wave2\n",
      "8    3   40   35895  wave_3  wave3\n",
      "9    4   60   65920  wave_1  wave1\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "id": "section_preparation",
   "metadata": {},
   "source": "<a id='preparation'></a>\n# 6. Harmonization & Pre-Analysis Preparation\n\nTest functions for harmonizing variables, building datasets, and preparing data for statistical analysis."
  },
  {
   "cell_type": "code",
   "id": "test_preparation_imports",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T22:49:47.150778Z",
     "start_time": "2025-10-16T22:49:47.146162Z"
    }
   },
   "source": [
    "import importlib\n",
    "from sociopathit.data import preparation as preparation_module\n",
    "importlib.reload(preparation_module)\n",
    "from sociopathit.data.preparation import (\n",
    "    harmonize_columns,\n",
    "    build_harmonized_dataset,\n",
    "    to_categorical_ordered,\n",
    "    numeric_codes,\n",
    "    prepare_for_analysis\n",
    ")\n",
    "\n",
    "print(\"Preparation module loaded successfully\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparation module loaded successfully\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "test_harmonize_columns",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T22:49:47.200749Z",
     "start_time": "2025-10-16T22:49:47.192121Z"
    }
   },
   "source": [
    "# Test 11: harmonize_columns\n",
    "print(\"=\" * 60)\n",
    "print(\"TEST 11: Harmonize Columns\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create test data with inconsistent labels\n",
    "df_inconsistent = pd.DataFrame({\n",
    "    'pid': range(1, 101),\n",
    "    'satisfaction': np.random.choice(\n",
    "        ['very satisfied', 'satisfied', 'neutral', 'dissatisfied', 'very dissatisfied'],\n",
    "        100\n",
    "    ),\n",
    "    'agreement': np.random.choice(\n",
    "        ['strongly agree', 'agree', 'neutral', 'disagree', 'strongly disagree', 'dk'],\n",
    "        100\n",
    "    )\n",
    "})\n",
    "\n",
    "print(\"Before harmonization:\")\n",
    "print(f\"Satisfaction values: {df_inconsistent['satisfaction'].unique()}\")\n",
    "print(f\"Agreement values: {df_inconsistent['agreement'].unique()}\")\n",
    "\n",
    "# Define mappings\n",
    "mappings = {\n",
    "    'satisfaction': {\n",
    "        'very satisfied': 'Very satisfied',\n",
    "        'satisfied': 'Satisfied',\n",
    "        'neutral': 'Neutral',\n",
    "        'dissatisfied': 'Dissatisfied',\n",
    "        'very dissatisfied': 'Very dissatisfied'\n",
    "    },\n",
    "    'agreement': {\n",
    "        'strongly agree': 'Strongly agree',\n",
    "        'agree': 'Agree',\n",
    "        'neutral': 'Neutral',\n",
    "        'disagree': 'Disagree',\n",
    "        'strongly disagree': 'Strongly disagree'\n",
    "    }\n",
    "}\n",
    "\n",
    "df_harmonized = harmonize_columns(df_inconsistent, mappings, handle_missing='na')\n",
    "\n",
    "print(\"\\nAfter harmonization:\")\n",
    "print(f\"Satisfaction values: {df_harmonized['satisfaction'].unique()}\")\n",
    "print(f\"Agreement values: {df_harmonized['agreement'].dropna().unique()}\")\n",
    "print(f\"\\nShape before: {df_inconsistent.shape}, after: {df_harmonized.shape}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TEST 11: Harmonize Columns\n",
      "============================================================\n",
      "Before harmonization:\n",
      "Satisfaction values: ['satisfied' 'neutral' 'very satisfied' 'very dissatisfied' 'dissatisfied']\n",
      "Agreement values: ['strongly disagree' 'disagree' 'neutral' 'strongly agree' 'agree' 'dk']\n",
      "\n",
      "After harmonization:\n",
      "Satisfaction values: ['Satisfied' 'Neutral' 'Very satisfied' 'Very dissatisfied' 'Dissatisfied']\n",
      "Agreement values: ['Strongly disagree' 'Disagree' 'Neutral' 'Strongly agree' 'Agree']\n",
      "\n",
      "Shape before: (100, 3), after: (100, 3)\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "id": "test_build_harmonized",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T22:49:47.248956Z",
     "start_time": "2025-10-16T22:49:47.238417Z"
    }
   },
   "source": [
    "# Test 12: build_harmonized_dataset\n",
    "print(\"=\" * 60)\n",
    "print(\"TEST 12: Build Harmonized Dataset\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create multi-wave data\n",
    "wave_data = {\n",
    "    'survey_2020': pd.DataFrame({\n",
    "        'pid': range(1, 101),\n",
    "        'age': np.random.randint(18, 65, 100),\n",
    "        'income': np.random.randint(30000, 100000, 100),\n",
    "        'satisfaction': np.random.randint(1, 8, 100),\n",
    "        'extra_2020': np.random.random(100)\n",
    "    }),\n",
    "    'survey_2021': pd.DataFrame({\n",
    "        'pid': range(1, 101),\n",
    "        'age': np.random.randint(18, 65, 100),\n",
    "        'income': np.random.randint(30000, 100000, 100),\n",
    "        'satisfaction': np.random.randint(1, 8, 100),\n",
    "        'extra_2021': np.random.random(100)\n",
    "    }),\n",
    "    'survey_2022': pd.DataFrame({\n",
    "        'pid': range(1, 101),\n",
    "        'age': np.random.randint(18, 65, 100),\n",
    "        'income': np.random.randint(30000, 100000, 100),\n",
    "        'satisfaction': np.random.randint(1, 8, 100),\n",
    "        'extra_2022': np.random.random(100)\n",
    "    })\n",
    "}\n",
    "\n",
    "harmonized = build_harmonized_dataset(\n",
    "    wave_data,\n",
    "    target_vars=['age', 'income', 'satisfaction'],\n",
    "    id_col='pid',\n",
    "    add_identifiers=True\n",
    ")\n",
    "\n",
    "print(f\"\\nHarmonized dataset shape: {harmonized.shape}\")\n",
    "print(f\"Columns: {list(harmonized.columns)}\")\n",
    "print(f\"\\nUnique surveys: {harmonized['surveyid'].unique()}\")\n",
    "print(f\"Wave labels: {harmonized['wave'].unique()}\")\n",
    "print(f\"\\nSample data:\")\n",
    "print(harmonized.head(10))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TEST 12: Build Harmonized Dataset\n",
      "============================================================\n",
      "\n",
      "Harmonized dataset shape: (300, 7)\n",
      "Columns: ['pid', 'age', 'income', 'satisfaction', 'surveyid', 'year', 'wave']\n",
      "\n",
      "Unique surveys: ['survey_2020' 'survey_2021' 'survey_2022']\n",
      "Wave labels: ['wave_1' 'wave_2' 'wave_3']\n",
      "\n",
      "Sample data:\n",
      "   pid  age  income  satisfaction     surveyid  year    wave\n",
      "0    1   60   57355             4  survey_2020  2020  wave_1\n",
      "1    2   28   34835             5  survey_2020  2020  wave_1\n",
      "2    3   35   50159             2  survey_2020  2020  wave_1\n",
      "3    4   64   77605             6  survey_2020  2020  wave_1\n",
      "4    5   29   68088             5  survey_2020  2020  wave_1\n",
      "5    6   26   85284             2  survey_2020  2020  wave_1\n",
      "6    7   27   87043             5  survey_2020  2020  wave_1\n",
      "7    8   61   65547             6  survey_2020  2020  wave_1\n",
      "8    9   34   57532             3  survey_2020  2020  wave_1\n",
      "9   10   55   64349             3  survey_2020  2020  wave_1\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "id": "test_categorical_ordered",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T22:49:47.301509Z",
     "start_time": "2025-10-16T22:49:47.295676Z"
    }
   },
   "source": [
    "# Test 13: to_categorical_ordered\n",
    "print(\"=\" * 60)\n",
    "print(\"TEST 13: Convert to Ordered Categorical\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "df_test = pd.DataFrame({\n",
    "    'response': ['Agree', 'Disagree', 'Strongly agree', 'Neutral', 'Strongly disagree'] * 20\n",
    "})\n",
    "\n",
    "order = ['Strongly disagree', 'Disagree', 'Neutral', 'Agree', 'Strongly agree']\n",
    "\n",
    "print(\"Before conversion:\")\n",
    "print(f\"  Type: {df_test['response'].dtype}\")\n",
    "print(f\"  Sample: {df_test['response'].head()}\")\n",
    "\n",
    "df_ordered = to_categorical_ordered(df_test, 'response', order)\n",
    "\n",
    "print(\"\\nAfter conversion:\")\n",
    "print(f\"  Type: {df_ordered['response'].dtype}\")\n",
    "print(f\"  Ordered: {df_ordered['response'].dtype.ordered}\")\n",
    "print(f\"  Categories: {list(df_ordered['response'].cat.categories)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TEST 13: Convert to Ordered Categorical\n",
      "============================================================\n",
      "Before conversion:\n",
      "  Type: object\n",
      "  Sample: 0                Agree\n",
      "1             Disagree\n",
      "2       Strongly agree\n",
      "3              Neutral\n",
      "4    Strongly disagree\n",
      "Name: response, dtype: object\n",
      "\n",
      "After conversion:\n",
      "  Type: category\n",
      "  Ordered: True\n",
      "  Categories: ['Strongly disagree', 'Disagree', 'Neutral', 'Agree', 'Strongly agree']\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "id": "test_numeric_codes",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T22:49:47.378316Z",
     "start_time": "2025-10-16T22:49:47.372675Z"
    }
   },
   "source": [
    "# Test 14: numeric_codes\n",
    "print(\"=\" * 60)\n",
    "print(\"TEST 14: Convert to Numeric Codes\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Use ordered categorical from previous test\n",
    "numeric = numeric_codes(df_ordered['response'], start=1)\n",
    "\n",
    "print(\"Original vs. Numeric:\")\n",
    "comparison = pd.DataFrame({\n",
    "    'original': df_ordered['response'].head(10),\n",
    "    'numeric': numeric.head(10)\n",
    "})\n",
    "print(comparison)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TEST 14: Convert to Numeric Codes\n",
      "============================================================\n",
      "Original vs. Numeric:\n",
      "            original  numeric\n",
      "0              Agree      4.0\n",
      "1           Disagree      2.0\n",
      "2     Strongly agree      5.0\n",
      "3            Neutral      3.0\n",
      "4  Strongly disagree      1.0\n",
      "5              Agree      4.0\n",
      "6           Disagree      2.0\n",
      "7     Strongly agree      5.0\n",
      "8            Neutral      3.0\n",
      "9  Strongly disagree      1.0\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "id": "test_prepare_for_analysis",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T22:49:47.466794Z",
     "start_time": "2025-10-16T22:49:47.453608Z"
    }
   },
   "source": "# Test 15: prepare_for_analysis\nprint(\"=\" * 60)\nprint(\"TEST 15: Prepare for Analysis\")\nprint(\"=\" * 60)\n\n# Create messy data\ndf_messy = pd.DataFrame({\n    'PID': range(1, 201),\n    'Age': np.concatenate([np.random.randint(18, 65, 190), [-99] * 10]),  # Missing codes\n    'Income': np.concatenate([np.random.randint(30000, 100000, 180), [-99] * 20]),\n    'JobSat': np.random.choice(['Very satisfied', 'Satisfied', 'Neutral', 'Dissatisfied'], 200),\n    'Weight': np.random.uniform(0.5, 1.5, 200)\n})\n\n# Add some rows with lots of missing data\ndf_messy.loc[195:199, ['Age', 'Income', 'JobSat']] = np.nan\n\nprint(\"Before preparation:\")\nprint(f\"  Shape: {df_messy.shape}\")\nprint(f\"  Columns: {list(df_messy.columns)}\")\nprint(f\"  Missing -99 in Age: {(df_messy['Age'] == -99).sum()}\")\nprint(f\"  Weight sum: {df_messy['Weight'].sum():.2f}\")\n\ndf_clean = prepare_for_analysis(\n    df_messy,\n    satisfaction_vars=['jobsat'],\n    weight_col='weight',\n    normalize_weights=True,\n    min_valid_pct=0.6\n)\n\nprint(\"\\nAfter preparation:\")\nprint(f\"  Shape: {df_clean.shape}\")\nprint(f\"  Columns: {list(df_clean.columns)}\")\nprint(f\"  Missing -99 in age: {(df_clean['age'] == -99).sum()}\")\nprint(f\"  Weight sum: {df_clean['weight'].sum():.2f}\")\nprint(f\"  JobSat type: {df_clean['jobsat'].dtype}\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TEST 15: Prepare for Analysis\n",
      "============================================================\n",
      "Before preparation:\n",
      "  Shape: (200, 5)\n",
      "  Columns: ['PID', 'Age', 'Income', 'JobSat', 'Weight']\n",
      "  Missing -99 in Age: 5\n",
      "  Weight sum: 200.17\n",
      "Removed 5 rows with >40% missing data\n",
      "\n",
      "After preparation:\n",
      "  Shape: (195, 5)\n",
      "  Columns: ['pid', 'age', 'income', 'jobsat', 'weight']\n",
      "  Missing -99 in age: 0\n",
      "  Weight sum: 195.21\n",
      "  JobSat type: category\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "id": "i70bdtsx26",
   "source": "# 7. Enhanced Dynamic Data Functions\n\nTest new functions for dynamic data loading, ID normalization, and automated longitudinal dataset construction.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "tsrkm32ipqq",
   "source": "# Test 16: normalize_ids - ID normalization with .0 removal\nprint(\"=\" * 60)\nprint(\"TEST 16: Normalize IDs\")\nprint(\"=\" * 60)\n\nfrom sociopathit.data.loading import normalize_ids\n\n# Create test data with problematic IDs\ndf_messy_ids = pd.DataFrame({\n    'id': [1.0, 2.0, 3.0, 4.0, 5.0, np.nan, 7.0, 8.0, 9.0, 10.0],\n    'age': np.random.randint(18, 65, 10),\n    'value': np.random.random(10)\n})\n\nprint(\"Before normalization:\")\nprint(f\"  ID dtype: {df_messy_ids['id'].dtype}\")\nprint(f\"  Sample IDs: {df_messy_ids['id'].head().tolist()}\")\nprint(f\"  Missing IDs: {df_messy_ids['id'].isna().sum()}\")\n\n# Normalize with default settings\ndf_normalized = normalize_ids(df_messy_ids, id_cols='id', handle_missing='keep')\n\nprint(\"\\nAfter normalization (keep missing):\")\nprint(f\"  ID dtype: {df_normalized['id'].dtype}\")\nprint(f\"  Sample IDs: {df_normalized['id'].head().tolist()}\")\nprint(f\"  Missing IDs: {df_normalized['id'].isna().sum()}\")\n\n# Test with filling missing IDs\ndf_normalized_fill = normalize_ids(df_messy_ids, id_cols='id', handle_missing='fill')\n\nprint(\"\\nAfter normalization (fill missing):\")\nprint(f\"  Missing IDs: {df_normalized_fill['id'].isna().sum()}\")\nprint(f\"  All IDs: {sorted(df_normalized_fill['id'].tolist())}\")\n\n# Test with string IDs containing .0\ndf_string_ids = pd.DataFrame({\n    'id': ['123.0', '456.0', '789.0', '1011.0'],\n    'value': [1, 2, 3, 4]\n})\n\nprint(\"\\nString IDs with .0 suffix:\")\nprint(f\"  Before: {df_string_ids['id'].tolist()}\")\ndf_string_norm = normalize_ids(df_string_ids, id_cols='id')\nprint(f\"  After: {df_string_norm['id'].tolist()}\")\nprint(f\"  After dtype: {df_string_norm['id'].dtype}\")",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T22:49:47.546925Z",
     "start_time": "2025-10-16T22:49:47.537623Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TEST 16: Normalize IDs\n",
      "============================================================\n",
      "Before normalization:\n",
      "  ID dtype: float64\n",
      "  Sample IDs: [1.0, 2.0, 3.0, 4.0, 5.0]\n",
      "  Missing IDs: 1\n",
      "\n",
      "After normalization (keep missing):\n",
      "  ID dtype: Int64\n",
      "  Sample IDs: [1, 2, 3, 4, 5]\n",
      "  Missing IDs: 1\n",
      "\n",
      "After normalization (fill missing):\n",
      "  Missing IDs: 0\n",
      "  All IDs: [1, 2, 3, 4, 5, 7, 8, 9, 10, 11]\n",
      "\n",
      "String IDs with .0 suffix:\n",
      "  Before: ['123.0', '456.0', '789.0', '1011.0']\n",
      "  After: [123, 456, 789, 1011]\n",
      "  After dtype: Int64\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "id": "8wy4qigl3po",
   "source": "# Test 17: load_and_combine - Multiple combine methods\nprint(\"=\" * 60)\nprint(\"TEST 17: Load and Combine Datasets\")\nprint(\"=\" * 60)\n\nfrom sociopathit.data.loading import load_and_combine\n\nwith TemporaryDirectory() as tmpdir:\n    tmpdir = Path(tmpdir)\n    \n    # Create test datasets with different file types\n    for year in [2020, 2021, 2022]:\n        df_test = pd.DataFrame({\n            'pid': [i + 0.0 for i in range(1, 51)],  # IDs with .0\n            'age': np.random.randint(18, 65, 50),\n            'income': np.random.randint(30000, 100000, 50),\n            'satisfaction': np.random.randint(1, 8, 50)\n        })\n        df_test.to_csv(tmpdir / f\"survey_{year}.csv\", index=False)\n    \n    # Also create a test file to exclude\n    df_test.to_csv(tmpdir / \"survey_2023_test.csv\", index=False)\n    \n    print(\"Test A: Separate method (returns dict)\")\n    print(\"-\" * 60)\n    data_separate = load_and_combine(\n        tmpdir,\n        file_types=['csv'],\n        exclude_patterns=['*_test*'],\n        combine_method='separate',\n        normalize_id=True\n    )\n    \n    print(f\"  Loaded {len(data_separate)} datasets:\")\n    for survey_id, df in data_separate.items():\n        print(f\"    - {survey_id}: {df.shape}, ID dtype: {df['pid'].dtype}\")\n    \n    print(\"\\nTest B: Concat method (long-form)\")\n    print(\"-\" * 60)\n    data_concat = load_and_combine(\n        tmpdir,\n        file_types=['csv'],\n        exclude_patterns=['*_test*'],\n        target_vars=['pid', 'age', 'income', 'satisfaction'],\n        combine_method='concat',\n        normalize_id=True\n    )\n    \n    print(f\"  Combined shape: {data_concat.shape}\")\n    print(f\"  Columns: {list(data_concat.columns)}\")\n    print(f\"  Unique sources: {data_concat['source'].unique()}\")\n    print(f\"  Unique waves: {sorted(data_concat['wave'].dropna().unique())}\")\n    \n    print(\"\\nTest C: Merge method (wide-form)\")\n    print(\"-\" * 60)\n    data_merge = load_and_combine(\n        tmpdir,\n        file_types=['csv'],\n        exclude_patterns=['*_test*'],\n        target_vars=['age', 'income'],\n        combine_method='merge',\n        id_col='pid',\n        normalize_id=True\n    )\n    \n    print(f\"  Merged shape: {data_merge.shape}\")\n    print(f\"  Columns: {list(data_merge.columns)}\")\n    print(f\"  Sample data:\")\n    print(data_merge.head(3))",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T22:49:47.690062Z",
     "start_time": "2025-10-16T22:49:47.607318Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TEST 17: Load and Combine Datasets\n",
      "============================================================\n",
      "Test A: Separate method (returns dict)\n",
      "------------------------------------------------------------\n",
      "âœ“ Loaded 3 datasets\n",
      "  File types: csv=3\n",
      "âš  Skipped 1 files\n",
      "  - survey_2023_test.csv: matched exclusion pattern\n",
      "  Loaded 3 datasets:\n",
      "    - survey_2020: (50, 4), ID dtype: Int64\n",
      "    - survey_2021: (50, 4), ID dtype: Int64\n",
      "    - survey_2022: (50, 4), ID dtype: Int64\n",
      "\n",
      "Test B: Concat method (long-form)\n",
      "------------------------------------------------------------\n",
      "âœ“ Loaded 3 datasets\n",
      "  File types: csv=3\n",
      "âš  Skipped 1 files\n",
      "  - survey_2023_test.csv: matched exclusion pattern\n",
      "  Combined shape: (150, 6)\n",
      "  Columns: ['pid', 'age', 'income', 'satisfaction', 'source', 'wave']\n",
      "  Unique sources: ['survey_2020' 'survey_2021' 'survey_2022']\n",
      "  Unique waves: [np.int64(2020), np.int64(2021), np.int64(2022)]\n",
      "\n",
      "Test C: Merge method (wide-form)\n",
      "------------------------------------------------------------\n",
      "âœ“ Loaded 3 datasets\n",
      "  File types: csv=3\n",
      "âš  Skipped 1 files\n",
      "  - survey_2023_test.csv: matched exclusion pattern\n",
      "  Merged shape: (50, 7)\n",
      "  Columns: ['pid', 'age_survey_2020', 'income_survey_2020', 'age_survey_2021', 'income_survey_2021', 'age_survey_2022', 'income_survey_2022']\n",
      "  Sample data:\n",
      "   pid  age_survey_2020  income_survey_2020  age_survey_2021  \\\n",
      "0    1               38               58228               59   \n",
      "1    2               47               64521               37   \n",
      "2    3               41               56928               23   \n",
      "\n",
      "   income_survey_2021  age_survey_2022  income_survey_2022  \n",
      "0               54217               45               76403  \n",
      "1               62994               37               39168  \n",
      "2               45036               62               79879  \n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "id": "9j81l0tqpup",
   "source": "# Test 18: extract_wave_info with directory path\nprint(\"=\" * 60)\nprint(\"TEST 18: Extract Wave Info from Directory\")\nprint(\"=\" * 60)\n\nfrom sociopathit.data.metadata import extract_wave_info\n\nwith TemporaryDirectory() as tmpdir:\n    tmpdir = Path(tmpdir)\n    \n    # Create test data files with date information in names\n    for year, month in [(2020, 'jan'), (2020, 'jun'), (2021, 'jan'), (2021, 'dec'), (2022, 'mar')]:\n        df_test = pd.DataFrame({\n            'pid': range(1, 51),\n            'value': np.random.random(50)\n        })\n        df_test.to_csv(tmpdir / f\"qwels_{month}_{year}.csv\", index=False)\n    \n    print(\"Test A: Extract from directory path\")\n    print(\"-\" * 60)\n    wave_info = extract_wave_info(tmpdir)\n    \n    print(f\"\\nExtracted wave information from {len(wave_info)} files:\")\n    print(wave_info.to_string(index=False))\n    \n    print(\"\\nTest B: Extract from dict of DataFrames\")\n    print(\"-\" * 60)\n    data_dict = {\n        'survey_wave_1': pd.DataFrame({'pid': range(1, 51)}),\n        'survey_wave_2': pd.DataFrame({'pid': range(1, 51)}),\n        'survey_wave_3': pd.DataFrame({'pid': range(1, 51)}),\n    }\n    \n    wave_info_dict = extract_wave_info(data_dict)\n    print(wave_info_dict.to_string(index=False))\n    \n    print(\"\\nTest C: Extract from list of IDs\")\n    print(\"-\" * 60)\n    survey_ids = ['study_2020', 'study_2021', 'study_2022']\n    wave_info_list = extract_wave_info(survey_ids)\n    print(wave_info_list.to_string(index=False))",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T22:49:47.766312Z",
     "start_time": "2025-10-16T22:49:47.748700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TEST 18: Extract Wave Info from Directory\n",
      "============================================================\n",
      "Test A: Extract from directory path\n",
      "------------------------------------------------------------\n",
      "\n",
      "Extracted wave information from 5 files:\n",
      "     survey_id  year month date wave_number wave_id\n",
      "qwels_jan_2020  2020   jan None        None  wave_1\n",
      "qwels_jun_2020  2020   jun None        None  wave_2\n",
      "qwels_dec_2021  2021   dec None        None  wave_3\n",
      "qwels_jan_2021  2021   jan None        None  wave_4\n",
      "qwels_mar_2022  2022   mar None        None  wave_5\n",
      "\n",
      "Test B: Extract from dict of DataFrames\n",
      "------------------------------------------------------------\n",
      "    survey_id year month date  wave_number wave_id\n",
      "survey_wave_1 None  None None            1  wave_1\n",
      "survey_wave_2 None  None None            2  wave_2\n",
      "survey_wave_3 None  None None            3  wave_3\n",
      "\n",
      "Test C: Extract from list of IDs\n",
      "------------------------------------------------------------\n",
      " survey_id  year month date wave_number wave_id\n",
      "study_2020  2020  None None        None  wave_1\n",
      "study_2021  2021  None None        None  wave_2\n",
      "study_2022  2022  None None        None  wave_3\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "id": "gvj2rzkhuvl",
   "source": "# Test 19: build_longit_from_dir - Complete workflow\nprint(\"=\" * 60)\nprint(\"TEST 19: Build Longitudinal Dataset from Directory\")\nprint(\"=\" * 60)\n\nfrom sociopathit.data.longitudinal import build_longit_from_dir\n\nwith TemporaryDirectory() as tmpdir:\n    tmpdir = Path(tmpdir)\n    \n    # Create longitudinal data files (same individuals across waves)\n    np.random.seed(42)\n    shared_pids = list(range(1, 81))  # 80 core individuals\n    \n    for year in [2020, 2021, 2022]:\n        # Add some new respondents in each wave\n        if year == 2021:\n            wave_pids = shared_pids + list(range(81, 91))\n        elif year == 2022:\n            wave_pids = shared_pids + list(range(91, 101))\n        else:\n            wave_pids = shared_pids\n        \n        df_wave = pd.DataFrame({\n            'pid': [float(p) for p in wave_pids],  # IDs with .0 \n            'age': np.random.randint(18, 65, len(wave_pids)),\n            'income': np.random.randint(30000, 100000, len(wave_pids)),\n            'satisfaction': np.random.randint(1, 8, len(wave_pids)),\n            'health': np.random.randint(1, 6, len(wave_pids)),\n            'extra_var': np.random.random(len(wave_pids))  # This won't be common\n        })\n        df_wave.to_csv(tmpdir / f\"survey_wave_{year}.csv\", index=False)\n    \n    # Also create a file to exclude\n    df_wave.to_csv(tmpdir / \"backup_survey.csv\", index=False)\n    \n    print(\"Building longitudinal dataset from directory...\")\n    print(\"-\" * 60)\n    \n    # Build long-form dataset\n    long_df = build_longit_from_dir(\n        data_dir=tmpdir,\n        target_vars=['age', 'income', 'satisfaction', 'health'],\n        id_col='pid',\n        file_types=['csv'],\n        exclude_patterns=['backup_*'],\n        normalize_ids=True,\n        auto_detect_waves=True\n    )\n    \n    print(f\"\\nâœ“ Longitudinal Dataset Created\")\n    print(f\"  Shape: {long_df.shape}\")\n    print(f\"  Columns: {list(long_df.columns)}\")\n    print(f\"  Unique individuals: {long_df['pid'].nunique()}\")\n    print(f\"  Waves: {sorted(long_df['wave'].unique())}\")\n    print(f\"  ID dtype: {long_df['pid'].dtype}\")\n    \n    print(f\"\\n  Wave distribution:\")\n    print(long_df['wave'].value_counts().sort_index())\n    \n    print(f\"\\n  Sample data (first individual across waves):\")\n    print(long_df[long_df['pid'] == 1])\n    \n    print(f\"\\n  Verify ID normalization (no .0 issues):\")\n    print(f\"    Sample IDs: {long_df['pid'].head(10).tolist()}\")",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T22:49:47.963028Z",
     "start_time": "2025-10-16T22:49:47.861605Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TEST 19: Build Longitudinal Dataset from Directory\n",
      "============================================================\n",
      "Building longitudinal dataset from directory...\n",
      "------------------------------------------------------------\n",
      "Loading data from C:\\Users\\alecw\\AppData\\Local\\Temp\\tmp4cqyuax1...\n",
      "âœ“ Loaded 3 datasets\n",
      "  File types: csv=3\n",
      "âš  Skipped 1 files\n",
      "  - backup_survey.csv: matched exclusion pattern\n",
      "Detecting wave structure...\n",
      "Using ID column: pid\n",
      "Normalizing IDs...\n",
      "Building long-form dataset...\n",
      "âœ“ Created long-form dataset: 260 observations\n",
      "  - 100 unique individuals\n",
      "  - 3 waves\n",
      "  - 4 variables\n",
      "\n",
      "âœ“ Longitudinal Dataset Created\n",
      "  Shape: (260, 7)\n",
      "  Columns: ['pid', 'age', 'income', 'satisfaction', 'health', 'wave', 'source']\n",
      "  Unique individuals: 100\n",
      "  Waves: ['wave_1', 'wave_2', 'wave_3']\n",
      "  ID dtype: Int64\n",
      "\n",
      "  Wave distribution:\n",
      "wave\n",
      "wave_1    80\n",
      "wave_2    90\n",
      "wave_3    90\n",
      "Name: count, dtype: int64\n",
      "\n",
      "  Sample data (first individual across waves):\n",
      "   pid  age  income  satisfaction  health    wave            source\n",
      "0    1   56   53247             7       2  wave_1  survey_wave_2020\n",
      "1    1   43   47955             1       1  wave_2  survey_wave_2021\n",
      "2    1   53   50611             1       5  wave_3  survey_wave_2022\n",
      "\n",
      "  Verify ID normalization (no .0 issues):\n",
      "    Sample IDs: [1, 1, 1, 2, 2, 2, 3, 3, 3, 4]\n"
     ]
    }
   ],
   "execution_count": 25
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
